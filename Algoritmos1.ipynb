{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vtponciano/ARTIGO-P3-INDEPENDENTE/blob/master/Algoritmos1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UmHVz6YbU2nx",
        "outputId": "6e5ee73d-1a33-4581-ad67-7a7cf3bfcad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamanho do Grafo: 84\n",
            "Tamanho do fecho inicial: 43\n",
            "Tamanho dos Gerados: 84\n",
            "Tamanho do fecho final: 84, Tempo final: 11\n",
            "Tamanho do Grafo: 84\n",
            "Tamanho do fecho inicial: 43\n",
            "Tamanho dos Gerados: 84\n",
            "Tamanho do fecho final: 84, Tempo final: 11\n",
            "Tamanho do Grafo: 84\n",
            "Tamanho do fecho inicial: 43\n",
            "Tamanho dos Gerados: 84\n",
            "Tamanho do fecho final: 84, Tempo final: 11\n",
            "Tamanho do Grafo: 84\n",
            "Tamanho do fecho inicial: 43\n",
            "Tamanho dos Gerados: 84\n",
            "Tamanho do fecho final: 84, Tempo final: 11\n",
            "Tamanho do Grafo: 84\n",
            "Tamanho do fecho inicial: 43\n",
            "Tamanho dos Gerados: 84\n",
            "Tamanho do fecho final: 84, Tempo final: 11\n",
            "Tamanho do Grafo: 84\n",
            "Tamanho do fecho inicial: 43\n",
            "Tamanho dos Gerados: 84\n",
            "Tamanho do fecho final: 84, Tempo final: 11\n",
            "Tamanho do Grafo: 84\n",
            "Tamanho do fecho inicial: 43\n",
            "Tamanho dos Gerados: 84\n",
            "Tamanho do fecho final: 84, Tempo final: 11\n",
            "Tamanho do Grafo: 84\n",
            "Tamanho do fecho inicial: 43\n",
            "Tamanho dos Gerados: 84\n",
            "Tamanho do fecho final: 84, Tempo final: 11\n",
            "Tamanho do Grafo: 84\n",
            "Tamanho do fecho inicial: 43\n",
            "Tamanho dos Gerados: 84\n",
            "Tamanho do fecho final: 84, Tempo final: 11\n",
            "Tamanho do Grafo: 84\n",
            "Tamanho do fecho inicial: 43\n",
            "Tamanho dos Gerados: 84\n",
            "Tamanho do fecho final: 84, Tempo final: 11\n",
            "Best Hull Set: {14, 22, 28, 31, 37, 38, 42, 44, 46, 48, 49, 50, 51, 52, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84}\n",
            "Best Hull Size tip_decomp_P3_Random: 43\n",
            "Tamanho do Grafo: 84\n",
            "Tamanho do fecho inicial: 43\n",
            "{64, 33, 39, 40, 41, 10, 7, 45, 53, 54, 27}\n",
            "{32, 36, 43, 47, 19, 20, 25, 29}\n",
            "{34, 35, 11, 15, 21, 30}\n",
            "{24, 17, 26, 6}\n",
            "{16, 18, 23}\n",
            "{12, 13}\n",
            "{8, 9}\n",
            "{4, 5}\n",
            "{2, 3}\n",
            "{1}\n",
            "set()\n",
            "Tamanho dos Gerados: 84\n",
            "Tamanho do fecho final: 84, Tempo final: 11\n",
            "Best Hull Set: {14, 22, 28, 31, 37, 38, 42, 44, 46, 48, 49, 50, 51, 52, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84}\n",
            "Best Hull Size MTS: 43\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import networkx as nx\n",
        "import random\n",
        "import os\n",
        "\n",
        "def threshold(v, G):\n",
        "      return 2\n",
        "\n",
        "def contaminar(contaminado, vcontaminante, vcontaminado):\n",
        "    if vcontaminado in contaminado:\n",
        "        if contaminado[vcontaminado] is None:\n",
        "            return False\n",
        "        if len(contaminado[vcontaminado]) < 2 and vcontaminante not in contaminado[vcontaminado]:\n",
        "            contaminado[vcontaminado].add(vcontaminante)\n",
        "            if len(contaminado[vcontaminado]) == 2:\n",
        "                return True\n",
        "    else:\n",
        "        contaminado[vcontaminado] = set()\n",
        "        contaminado[vcontaminado].add(vcontaminante)\n",
        "        if len(contaminado[vcontaminado]) == 2:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def init_contaminado(fecho):\n",
        "    contaminado = {}\n",
        "    for v in fecho:\n",
        "        contaminado[v] = None\n",
        "    return contaminado\n",
        "\n",
        "def get_novo_fecho(fecho, graph, contaminado):\n",
        "    novo_fecho = set()\n",
        "    for v in fecho:\n",
        "        if v in graph:\n",
        "            for w in graph[v]:\n",
        "                novo_contaminado = contaminar(contaminado, v, w)\n",
        "                if novo_contaminado:\n",
        "                    novo_fecho.add(w)\n",
        "    return novo_fecho\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def MTS(G, t):\n",
        "    V = set()\n",
        "    L = set()\n",
        "    U = set(G.nodes())\n",
        "\n",
        "    k = {v: t[v] for v in G}\n",
        "    delta = {v: G.degree(v) for v in G}\n",
        "\n",
        "    for v in U:\n",
        "        if G.degree(v) == 1:\n",
        "            V.add(v)\n",
        "\n",
        "    while U:\n",
        "        if any(k[v] == 0 for v in U):\n",
        "            eligible_nodes = [v for v in U if k[v] == 0]\n",
        "            v = random.choice(eligible_nodes)\n",
        "\n",
        "            for u in set(G.neighbors(v)) & U:\n",
        "                k[u] = max(k[u] - 1, 0)\n",
        "\n",
        "                if v not in L:\n",
        "                    delta[u] -= 1\n",
        "\n",
        "            U.remove(v)\n",
        "\n",
        "        else:\n",
        "            candidates_1 = [v for v in U - L if delta[v] < k[v]]\n",
        "\n",
        "            if candidates_1:\n",
        "                v = random.choice(candidates_1)\n",
        "\n",
        "                V.add(v)\n",
        "\n",
        "                for u in set(G.neighbors(v)) & U:\n",
        "                    k[u] -= 1\n",
        "                    delta[u] -= 1\n",
        "\n",
        "                U.remove(v)\n",
        "\n",
        "            else:\n",
        "                v = None\n",
        "                candidates_2 = [u for u in U - L]\n",
        "\n",
        "                if candidates_2:\n",
        "                    min_candidate = min(candidates_2, key=lambda u: (k[u], delta[u] * (delta[u] + 1)))\n",
        "                    v = random.choice([min_candidate])\n",
        "\n",
        "                for u in set(G.neighbors(v)) & U:\n",
        "                    delta[u] -= 1\n",
        "\n",
        "                L.add(v)\n",
        "\n",
        "    fecho = set(V)  # Initialize fecho with the remaining vertices in V\n",
        "    print(\"Tamanho do Grafo:\", len(G.nodes()))\n",
        "    print(\"Tamanho do fecho inicial:\", len(fecho))\n",
        "\n",
        "    # Some code related to init_contaminado and get_novo_fecho is missing, please define or import them.\n",
        "    contaminado = init_contaminado(fecho)\n",
        "    time = 0\n",
        "    while True:\n",
        "        G_contamination = G\n",
        "        novo_fecho = get_novo_fecho(fecho, G, contaminado)\n",
        "        print(novo_fecho)\n",
        "        fecho.update(novo_fecho)  # Add the new vertices to fecho\n",
        "        contaminated_vertices = novo_fecho  # Use contaminado instead of novo_fecho\n",
        "        for v in contaminated_vertices:\n",
        "            G_contamination.nodes[v]['contaminated_at_time'] = time\n",
        "        time += 1\n",
        "\n",
        "        if len(novo_fecho) == 0:\n",
        "            break\n",
        "\n",
        "\n",
        "    # Write the graph to a GEXF file\n",
        "    nx.write_gexf(G_contamination, \"contamination_graph.gexf\")\n",
        "\n",
        "    print(\"Tamanho dos Gerados:\", len(fecho))\n",
        "    print(\"Tamanho do fecho final: {}, Tempo final: {}\".format(len(fecho), time))\n",
        "\n",
        "    return V\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import networkx as nx\n",
        "import random\n",
        "import os\n",
        "\n",
        "def threshold(v, G):\n",
        "      return 2\n",
        "\n",
        "def init_contaminado(fecho):\n",
        "    contaminado = {}\n",
        "    for v in fecho:\n",
        "        contaminado[v] = None\n",
        "    return contaminado\n",
        "\n",
        "def get_novo_fecho(fecho, graph, contaminado):\n",
        "    novo_fecho = set()\n",
        "    for v in fecho:\n",
        "        if v in graph:\n",
        "            for w in graph[v]:\n",
        "                novo_contaminado = contaminar(contaminado, v, w)\n",
        "                if novo_contaminado:\n",
        "                    novo_fecho.add(w)\n",
        "    return novo_fecho\n",
        "\n",
        "def contaminar(contaminado, vcontaminante, vcontaminado):\n",
        "    if vcontaminado in contaminado:\n",
        "        if contaminado[vcontaminado] is None:\n",
        "            return False\n",
        "        if len(contaminado[vcontaminado]) < 2 and vcontaminante not in contaminado[vcontaminado]:\n",
        "            contaminado[vcontaminado].add(vcontaminante)\n",
        "            if len(contaminado[vcontaminado]) == 2:\n",
        "                return True\n",
        "    else:\n",
        "        contaminado[vcontaminado] = set()\n",
        "        contaminado[vcontaminado].add(vcontaminante)\n",
        "        if len(contaminado[vcontaminado]) == 2:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def tip_decomp_P3_Random(threshold, G):\n",
        "    V = set(G.nodes())\n",
        "    dist = {}\n",
        "    activated_nodes = set()\n",
        "\n",
        "    # Incluir todos os vértices no conjunto inicial V'\n",
        "    for vi in V:\n",
        "        ki = threshold(vi, G)\n",
        "        dist[vi] = G.degree(vi) - ki\n",
        "        if G.degree(vi) == 1:\n",
        "            dist[vi] = float('inf')\n",
        "\n",
        "    flag = True\n",
        "    while flag:\n",
        "        min_value = min(dist.values())\n",
        "        min_nodes = [node for node, value in dist.items() if value == min_value]\n",
        "        vi = random.choice(min_nodes)\n",
        "\n",
        "\n",
        "\n",
        "        if dist[vi] == float('inf'):\n",
        "            flag = False\n",
        "        else:\n",
        "            V.remove(vi)\n",
        "            del dist[vi]  # Remover o nó do dicionário dist\n",
        "            neighbors = list(G.neighbors(vi))\n",
        "            for vj in neighbors:\n",
        "                if vj in dist:\n",
        "                    if dist[vj] > 0:\n",
        "                        dist[vj] -= 1\n",
        "                    else:\n",
        "                        dist[vj] = float('inf')\n",
        "\n",
        "\n",
        "    fecho = set(V)  # Inicializar fecho com os vértices restantes em V\n",
        "    print(\"Tamanho do Grafo:\", len(G.nodes()))\n",
        "    print(\"Tamanho do fecho inicial:\", len(fecho))\n",
        "\n",
        "    contaminado = init_contaminado(fecho)\n",
        "    t = 0\n",
        "    while True:\n",
        "        novo_fecho = get_novo_fecho(fecho, G, contaminado)\n",
        "        fecho.update(novo_fecho)  # Adicionar os novos vértices ao fecho\n",
        "        t += 1\n",
        "        if len(novo_fecho) == 0:\n",
        "            break\n",
        "    print(\"Tamanho dos Gerados:\", len(fecho))\n",
        "    print(\"Tamanho do fecho final: {}, Tempo final: {}\".format(len(fecho), t))\n",
        "\n",
        "    return V\n",
        "\n",
        "\n",
        "# Criar um grafo não direcionado vazio\n",
        "G = nx.Graph()\n",
        "# Check if the file exists\n",
        "filename = 'treE13.tgf'\n",
        "if not os.path.isfile(filename):\n",
        "    print(f\"File '{filename}' does not exist.\")\n",
        "    # Handle the error or exit the program as needed\n",
        "else:\n",
        "    # Read the edges from the file\n",
        "    with open(filename, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line:  # Check if the line is not empty\n",
        "                vertices = line.split()\n",
        "                u, v = map(int, vertices[:2])  # Convert the first two values to integers\n",
        "                G.add_edge(u, v)\n",
        "\n",
        "def threshold(v, G):\n",
        "     return 2\n",
        "\n",
        "\n",
        "# Criar um dicionário com os limiares dos nós\n",
        "t = {v: threshold(v, G) for v in G}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Chamada da função tip_decomp\n",
        "\n",
        "best_hull_set = None\n",
        "best_hull_size = float('inf')\n",
        "\n",
        "for _ in range(10):\n",
        "    hull_set =  tip_decomp_P3_Random(threshold, G)\n",
        "    hull_size = len(hull_set)\n",
        "\n",
        "    if hull_size < best_hull_size:\n",
        "        best_hull_set = hull_set\n",
        "        best_hull_size = hull_size\n",
        "\n",
        "print(\"Best Hull Set:\", best_hull_set)\n",
        "print(\"Best Hull Size tip_decomp_P3_Random:\", best_hull_size)\n",
        "\n",
        "\n",
        "best_hull_set = None\n",
        "best_hull_size = float('inf')\n",
        "\n",
        "for _ in range(1):\n",
        "    hull_set = MTS(G,t)\n",
        "    hull_size = len(hull_set)\n",
        "\n",
        "    if hull_size < best_hull_size:\n",
        "        best_hull_set = hull_set\n",
        "        best_hull_size = hull_size\n",
        "\n",
        "print(\"Best Hull Set:\", best_hull_set)\n",
        "print(\"Best Hull Size MTS:\", best_hull_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwhqy27gh132",
        "outputId": "90a00a38-55c3-42aa-e40f-11078cca8933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.56.4)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.39.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.18 in /usr/local/lib/python3.10/dist-packages (from numba) (1.22.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba) (67.7.2)\n",
            "Requirement already satisfied: ninput in /usr/local/lib/python3.10/dist-packages (0.3.1)\n",
            "Requirement already satisfied: colab-env in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: python-dotenv<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from colab-env) (0.21.1)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "more: cannot open vars.env: No such file or directory\n",
            "minimum: 1, maximum: 41, n: 21\n",
            "tamanho do MELHOR FECHO INICIAL: 64\n",
            "numero de vertices alcancados pelo MELHOR FECHO INICIAL: 84\n",
            "tempo do MELHOR FECHO INICIAL: 2\n",
            "\n",
            "minimum: 1, maximum: 21, n: 11\n",
            "tamanho do MELHOR FECHO INICIAL: 54\n",
            "numero de vertices alcancados pelo MELHOR FECHO INICIAL: 84\n",
            "tempo do MELHOR FECHO INICIAL: 3\n",
            "\n",
            "minimum: 1, maximum: 11, n: 6\n",
            "tamanho do MELHOR FECHO INICIAL: 49\n",
            "numero de vertices alcancados pelo MELHOR FECHO INICIAL: 84\n",
            "tempo do MELHOR FECHO INICIAL: 4\n",
            "\n",
            "minimum: 1, maximum: 6, n: 3\n",
            "tamanho do MELHOR FECHO INICIAL: 46\n",
            "numero de vertices alcancados pelo MELHOR FECHO INICIAL: 84\n",
            "tempo do MELHOR FECHO INICIAL: 5\n",
            "\n",
            "minimum: 1, maximum: 3, n: 2\n",
            "tamanho do MELHOR FECHO INICIAL: 45\n",
            "numero de vertices alcancados pelo MELHOR FECHO INICIAL: 84\n",
            "tempo do MELHOR FECHO INICIAL: 6\n",
            "\n",
            "\n",
            "finalizado em 0.16684691699992982 segundos\n",
            "\n",
            "vertices do MELHOR FECHO INICIAL: <__main__.Hull object at 0x7dbbf26e0a00>\n",
            "tamanho do MELHOR FECHO INICIAL: 45\n",
            "numero de vertices alcancados pelo MELHOR FECHO INICIAL: 84\n",
            "tempo do MELHOR FECHO INICIAL: 6\n",
            "\n",
            "vertices do FECHO DE MELHOR TEMPO: <__main__.Hull object at 0x7dbbf26e2860>\n",
            "tamanho do FECHO DE MELHOR TEMPO: 64\n",
            "numero de vertices alcancados pelo MELHOR FECHO INICIAL: 84\n",
            "tempo do FECHO DE MELHOR TEMPO: 2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!pip install numba\n",
        "!pip install ninput\n",
        "!pip install colab-env --upgrade\n",
        "\n",
        "import math\n",
        "import os\n",
        "import networkx as nx\n",
        "import heapq\n",
        "import threading\n",
        "import colab_env\n",
        "from cmath import inf\n",
        "import math\n",
        "import random\n",
        "colab_env.RELOAD()\n",
        "!more vars.env\n",
        "import queue\n",
        "import timeit\n",
        "import numba as nb\n",
        "import csv\n",
        "import networkx as nx\n",
        "\n",
        "class Graph:\n",
        "    def __init__(self):\n",
        "        self.graph = nx.Graph()  # initialize a NetworkX graph\n",
        "\n",
        "    # Rest of the code...\n",
        "\n",
        "\n",
        "class Graph:\n",
        "    def __init__(self, path):\n",
        "        self.reset_graph()\n",
        "        self.read(path)\n",
        "        self.avl_hull = None\n",
        "        self.mnd_hull = None\n",
        "        self.mandatory_hull() # set mandatory hull previously\n",
        "        self.available_hull() # set available hull previously\n",
        "        # self.write_graph(path)\n",
        "\n",
        "    def reset_graph(self):\n",
        "        self.graph = {}\n",
        "        self.nedges = 0\n",
        "        self.vmax = 0\n",
        "        self.vmin = math.inf\n",
        "\n",
        "    def read(self, path):\n",
        "\n",
        "        self.path = f\"{path}.{os.getenv('FILE_INPUT_EXTENSION')}\"\n",
        "        with open(self.path) as f:\n",
        "            while True:\n",
        "                row = f.readline()\n",
        "                if not row:\n",
        "                    break\n",
        "                self.nedges += 1\n",
        "                if \"#\" in row:\n",
        "                    print('encontrado um #')\n",
        "                    self.reset_graph()\n",
        "                    continue\n",
        "                v1, v2 = int(row.split()[0]), int(row.split()[1])\n",
        "                self.vmin = min(self.vmin, v1, v2)\n",
        "                self.vmax = max(self.vmax, v1, v2)\n",
        "                self.add_on_adjacenty_list_undirected(v1, v2)\n",
        "\n",
        "    def __len__(self):\n",
        "        # o numero de vertices do grafo\n",
        "        # obs.: vertices nao encontrados na entrada (entre vmin e vmax) sao considerados como vertices isolados de grau 0\n",
        "        return self.vmax\n",
        "\n",
        "    def add_on_adjacenty_list_undirected(self, u, w):\n",
        "        self.add_on_adjacency_list(u, w)\n",
        "        self.add_on_adjacency_list(w, u)\n",
        "\n",
        "    def add_on_adjacency_list(self, u, w):\n",
        "        if u not in self.graph:\n",
        "            self.graph[u] = set()\n",
        "            self.graph[u].add(w)\n",
        "        else:\n",
        "            self.graph[u].add(w)\n",
        "\n",
        "    # @functools.lru_cache\n",
        "    def mandatory_hull(self):\n",
        "        if self.mnd_hull is None:\n",
        "            # conjunto com o vertices que necessariamente devem estar no fecho inicial pois de outra forma nao seriam contaminados\n",
        "            hull = Hull()\n",
        "            for i in range(self.vmin, self.vmax + 1):\n",
        "                if i not in self.graph: # vertice de grau 0\n",
        "                    hull.append(i)\n",
        "                elif len(self.graph[i]) < int(os.getenv('CONTAMINANTS')): # vertices de grau mais baixo que o numero de vizinhos necessarios para contaminar\n",
        "                    hull.append(i)\n",
        "            self.mnd_hull = hull\n",
        "        return self.mnd_hull\n",
        "\n",
        "    # @functools.lru_cache\n",
        "    def available_hull(self):\n",
        "        if self.avl_hull is None:\n",
        "            # conjunto de vertices que podem ser selecionados para serem parte de um hull inicial\n",
        "            hull = Hull()\n",
        "            for i in range(self.vmin, self.vmax + 1):\n",
        "                if i not in self.mandatory_hull():\n",
        "                    hull.append_with_weight(i, 1)\n",
        "            self.avl_hull = hull\n",
        "        return self.avl_hull\n",
        "\n",
        "    def evolve_hull(self, hull):\n",
        "        hullarray = []\n",
        "        for v in hull.last_border():\n",
        "            if v in self.graph:\n",
        "                for w in self.graph[v]:\n",
        "                    wasinfected = hull.infect(v, w)\n",
        "                    if wasinfected:\n",
        "                        hullarray.append(w)\n",
        "        hull.evolve(hullarray)\n",
        "        return hull\n",
        "\n",
        "    def hull_algorithm(self, hull):\n",
        "        last_hull_length = len(hull)\n",
        "        while True:\n",
        "            # print(\"t: {}, fecho: {}\".format(t, fecho))\n",
        "            hull = self.evolve_hull(hull)\n",
        "            if len(hull) == last_hull_length:\n",
        "                break\n",
        "            else:\n",
        "                last_hull_length = len(hull)\n",
        "            # print(\"novo_fecho: {}\".format(novo_fecho))\n",
        "        return hull\n",
        "\n",
        "\n",
        "\n",
        "    def save_hulls(self,hull_best, output_dir):\n",
        "     os.makedirs(output_dir, exist_ok=True)  # Cria o diretório de saída, se necessário\n",
        "\n",
        "     filename = os.path.join(output_dir, \"hull_best.txt\")\n",
        "\n",
        "     with open(filename, \"w\") as f:\n",
        "        f.write(\"Initial Hull:\\n\")\n",
        "        f.write(\",\".join(map(str, hull_best.initial_hull())) + \"\\n\\n\")\n",
        "\n",
        "        f.write(\"Final Hull:\\n\")\n",
        "        f.write(\",\".join(map(str, hull_best)) + \"\\n\")\n",
        "\n",
        "class Hull:\n",
        "    def __init__(self, array = None):\n",
        "        self.infection = {} # mostra por quais vertices um vertice foi contaminado\n",
        "        self.hull = []\n",
        "        self.weights = []\n",
        "        self.time = 0\n",
        "        self.times = {} # tempo em houve a entrada do vertice\n",
        "        self.times[self.time] = array or []\n",
        "        for v in array or []:\n",
        "            self.infection[v] = None\n",
        "            self.hull.append(v)\n",
        "\n",
        "    def append(self, other):\n",
        "        self.hull.append(other)\n",
        "        self.times[self.time].append(other)\n",
        "\n",
        "    def append_with_weight(self, other, weight):\n",
        "        self.hull.append(other)\n",
        "        self.weights.append(weight)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        return Hull(self.hull + other.hull)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hull)\n",
        "\n",
        "    def __contains__(self, key):\n",
        "        return key in self.hull\n",
        "\n",
        "    def __iter__(self):\n",
        "        for v in self.hull:\n",
        "            yield v\n",
        "\n",
        "    def weighted_selection_without_replacement(self, n):\n",
        "        # https://colab.research.google.com/drive/14Vnp-5xRHLZYE_WTczhpoMW2KdC6Cnvs#scrollTo=wEwWxLMKbpZn\n",
        "        elt = [(math.log(random.random()) / self.weights[i], i) for i in range(len(self.weights))]\n",
        "        return [x[1] for x in heapq.nlargest(n, elt)]\n",
        "\n",
        "    def random_subset(self, n, with_weight = False):\n",
        "        if with_weight:\n",
        "            indexes = self.weighted_selection_without_replacement(n)\n",
        "        else:\n",
        "            indexes = random.sample(range(len(self.hull)), n)\n",
        "        sample = [self.hull[i] for i in indexes]\n",
        "\n",
        "        return Hull(sample), indexes\n",
        "\n",
        "    def update_weights(self, indexes, internal = False):\n",
        "        if internal:\n",
        "            for i in indexes:\n",
        "                self.weights[i] *= int(os.getenv('VELOCITY'))\n",
        "        else:\n",
        "            sum_indexes_weights = sum(self.weights[i] for i in indexes)\n",
        "            sum_non_indexes = sum(weight for weight in self.weights)\n",
        "            remain = (((int(os.getenv('ONE_IN')) * sum_non_indexes) - sum_indexes_weights) + len(indexes)) // len(indexes)\n",
        "            for i in indexes:\n",
        "                self.weights[i] += remain\n",
        "        biggest = max(self.weights)\n",
        "        maximum = 1000000\n",
        "        minimum = 1/10000000000\n",
        "        if biggest > maximum: # normalize weights\n",
        "            self.weights = [max(weight * maximum / biggest, minimum) for weight in self.weights]\n",
        "\n",
        "    def evolve(self, array):\n",
        "        if array:\n",
        "            self.time += 1\n",
        "            self.times[self.time] = array\n",
        "            self.hull += array\n",
        "\n",
        "    def last_border(self):\n",
        "        return self.times[self.time]\n",
        "\n",
        "    \"\"\"\n",
        "    contaminado na chave \"i\" tem a lista de vértices que o contaminaram\n",
        "    os contaminados já no fecho inicial não tem uma lista e sim são iguais a None\n",
        "    retorna True se for um novo contaminado por 2 elementos\n",
        "    retorna False caso contrário\n",
        "    \"\"\"\n",
        "    def infect(self, vcontaminant, vcontaminated):\n",
        "        if vcontaminated in self.infection:\n",
        "            if self.infection[vcontaminated] is None:\n",
        "                return False\n",
        "            if len(self.infection[vcontaminated]) < int(os.getenv('CONTAMINANTS')) and vcontaminant not in self.infection[vcontaminated]:\n",
        "                self.infection[vcontaminated].add(vcontaminant)\n",
        "                if len(self.infection[vcontaminated]) == int(os.getenv('CONTAMINANTS')):\n",
        "                    return True\n",
        "        else:\n",
        "            self.infection[vcontaminated] = set()\n",
        "            self.infection[vcontaminated].add(vcontaminant)\n",
        "            if len(self.infection[vcontaminated]) == int(os.getenv('CONTAMINANTS')):\n",
        "                # só existe essa condição para no futuro podermos evoluir pra qualquer N >= 1, aqui no caso N=2\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def initial_hull(self):\n",
        "        return self.times[0]\n",
        "\n",
        "    def write(self, graph, path):\n",
        "        g = nx.Graph(graph.graph)\n",
        "        for i in range(graph.vmin, graph.vmax + 1):\n",
        "            g.add_node(i)\n",
        "        for time, array in self.times.items():\n",
        "            for i in array:\n",
        "                g.nodes[i]['Time'] = time\n",
        "        nx.write_gexf(g, f\"hull_{path}.gexf\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if os.getenv('PARALLEL') == 'True':\n",
        "        print(f\"numero de cpus detectados pelo ray: {ray._private.utils.get_num_cpus()}\")\n",
        "        # ray.init(num_cpus=12) # to increment cpu usage on ray\n",
        "\n",
        "\n",
        "def run_samples(graph, n):\n",
        "    first = True\n",
        "    for cnt in range(0, int(os.getenv('LENGTH_SAMPLE'))):\n",
        "        random_hull, idx = graph.available_hull().random_subset(n, os.getenv('WITH_WEIGHT') == 'True')\n",
        "        hull = graph.mandatory_hull() + random_hull\n",
        "\n",
        "        hull = graph.hull_algorithm(hull)\n",
        "\n",
        "        if first or (len(hull) > len(hull_best)) or (len(hull) == len(hull_best) and hull.time < hull_best.time):\n",
        "            if not first and os.getenv('WITH_WEIGHT') == 'True':\n",
        "                graph.available_hull().update_weights(indexes, True)\n",
        "            first = False\n",
        "            hull_best = hull\n",
        "            indexes = idx\n",
        "        if os.getenv('STOP_ON_FIRST_BEST_SAMPLE') == 'True' and reach_threshold(hull, len(graph)):\n",
        "            break\n",
        "    return hull_best, indexes\n",
        "\n",
        "\n",
        "def reach_threshold(hull, vmax):\n",
        "    return len(hull) == vmax\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def optimize(graph, flexible=False):\n",
        "    minimum = 1\n",
        "    maximum = len(graph.available_hull())\n",
        "    n = math.ceil(maximum / 2)\n",
        "    first_hull_time = True\n",
        "    first_hull = True\n",
        "    hull_best = None  # Inicializa a variável hull_best com None\n",
        "    hull_time = None  # Inicializa a variável hull_time com None\n",
        "\n",
        "    while True:\n",
        "        print('minimum: {}, maximum: {}, n: {}'.format(minimum, maximum, n))\n",
        "\n",
        "        if os.getenv('PARALLEL') == 'False':\n",
        "          hull, indexes = run_samples(graph, n)\n",
        "\n",
        "        if reach_threshold(hull, len(graph)):\n",
        "            if first_hull_time or (hull.time <= hull_time.time and len(hull.initial_hull()) < len(hull_time.initial_hull())):\n",
        "                first_hull_time = False\n",
        "                hull_time = hull\n",
        "\n",
        "            if first_hull or len(hull.initial_hull()) < len(hull_best.initial_hull()) or (len(hull.initial_hull()) == len(hull_best.initial_hull()) and hull.time < hull_best.time):\n",
        "                if not first_hull and os.getenv('WITH_WEIGHT') == 'True':\n",
        "                    graph.available_hull().update_weights(indexes)\n",
        "                first_hull = False\n",
        "                hull_best = hull\n",
        "                print(\"tamanho do MELHOR FECHO INICIAL: {}\".format(len(hull_best.initial_hull())))\n",
        "                print(\"numero de vertices alcancados pelo MELHOR FECHO INICIAL: {}\".format(len(hull_best)))\n",
        "                print(\"tempo do MELHOR FECHO INICIAL: {}\".format(hull_best.time))\n",
        "                print()\n",
        "\n",
        "                if flexible:\n",
        "                    minimum = 1\n",
        "            maximum = n\n",
        "            n = (maximum + minimum) // 2\n",
        "        else:\n",
        "            minimum = n\n",
        "            n = (n * 2)  # Muda a regra de atualização de n para busca exponencial\n",
        "        if maximum - minimum <= 1:\n",
        "            break\n",
        "    return hull_best, hull_time\n",
        "\n",
        "\n",
        "\n",
        "def exec():\n",
        "    graph = Graph(f\"{os.getenv('INITIAL_GRAPH')}\")\n",
        "    start = timeit.default_timer()\n",
        "\n",
        "    hull_best, hull_time = optimize(graph, os.getenv('FLEXIBLE_BINARY_SEARCH') == 'True')\n",
        "\n",
        "    stop = timeit.default_timer()\n",
        "    print(f'\\nfinalizado em {stop - start} segundos\\n')\n",
        "\n",
        "    print(\"vertices do MELHOR FECHO INICIAL: {}\".format(hull_best))\n",
        "    print(\"tamanho do MELHOR FECHO INICIAL: {}\".format(len(hull_best.initial_hull())))\n",
        "    print(\"numero de vertices alcancados pelo MELHOR FECHO INICIAL: {}\".format(len(hull_best)))\n",
        "    print(\"tempo do MELHOR FECHO INICIAL: {}\".format(hull_best.time))\n",
        "    print()\n",
        "    output_dir = \"./\"  # Caminho para o diretório atual\n",
        "    graph.save_hulls(hull_best, output_dir)\n",
        "\n",
        "    print(\"vertices do FECHO DE MELHOR TEMPO: {}\".format(hull_time))\n",
        "    print(\"tamanho do FECHO DE MELHOR TEMPO: {}\".format(len(hull_time.initial_hull())))\n",
        "    print(\"numero de vertices alcancados pelo MELHOR FECHO INICIAL: {}\".format(len(hull_time)))\n",
        "    print(\"tempo do FECHO DE MELHOR TEMPO: {}\".format(hull_time.time))\n",
        "\n",
        "    hull_best.write(graph, f\"best_{os.getenv('INITIAL_GRAPH')}\")\n",
        "    hull_time.write(graph, f\"time_{os.getenv('INITIAL_GRAPH')}\")\n",
        "\n",
        "\n",
        "def bulkexec():\n",
        "    first = True\n",
        "    for i in range(1, 5):\n",
        "        graphname = str(i).zfill(3)\n",
        "        graph = Graph(graphname)\n",
        "        start = timeit.default_timer()\n",
        "        hull_best, hull_time = optimize(graph, os.getenv('FLEXIBLE_BINARY_SEARCH') == 'True')\n",
        "        stop = timeit.default_timer()\n",
        "        exec_time = stop - start\n",
        "\n",
        "        hull_best.write(graph, f\"best_{graphname}\")\n",
        "        hull_time.write(graph, f\"time_{graphname}\")\n",
        "\n",
        "        dicts = {\n",
        "            'Graph': graphname,\n",
        "            'Time': int(exec_time),\n",
        "            'Len': len(hull_best.initial_hull()),\n",
        "            'Alcance': len(hull_best),\n",
        "            'T': hull_best.time,\n",
        "            'Len(hulltime)': len(hull_time.initial_hull()),\n",
        "            'Alcance(hulltime)': len(hull_time),\n",
        "            'T(hulltime)': hull_time.time,\n",
        "            'INITIAL_GRAPH': os.getenv('INITIAL_GRAPH'),\n",
        "            'CONTAMINANTS': os.getenv('CONTAMINANTS'),\n",
        "            'LENGTH_SAMPLE': os.getenv('LENGTH_SAMPLE'),\n",
        "            'STOP_ON_FIRST_BEST_SAMPLE': os.getenv('STOP_ON_FIRST_BEST_SAMPLE'),\n",
        "            'FLEXIBLE_BINARY_SEARCH': os.getenv('FLEXIBLE_BINARY_SEARCH'),\n",
        "            'WITH_WEIGHT': os.getenv('WITH_WEIGHT'),\n",
        "            'VELOCITY': os.getenv('VELOCITY'),\n",
        "            'PARALLEL': os.getenv('PARALLEL'),\n",
        "            'MAX_PARALLEL': os.getenv('MAX_PARALLEL'),\n",
        "            'ONE_IN': os.getenv('ONE_IN')\n",
        "        }\n",
        "        with open(f\"results.csv\", 'a', newline='') as output_file:\n",
        "            dict_writer = csv.DictWriter(output_file, dicts.keys())\n",
        "            if first:\n",
        "                dict_writer.writeheader()\n",
        "            dict_writer.writerow(dicts)\n",
        "            first = False\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if os.getenv('PARALLEL') == 'True':\n",
        "        print(f\"numero de cpus detectados pelo ray: {ray._private.utils.get_num_cpus()}\")\n",
        "        # ray.init(num_cpus=12) # to increment cpu usage on ray\n",
        "\n",
        "    exec()\n",
        "    # bulkexec()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlZBwh27VkZf",
        "outputId": "3a6131d8-ebbe-43db-d178-b5864db1e1bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O grafo foi convertido e salvo em 'grafo_Maculan.tgf' no formato Pajek.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "def convertir_json_a_pajecket(grafo_json):\n",
        "    nodos = {}\n",
        "    contenido_pajecket = []\n",
        "\n",
        "    # Extraer los nodos y asignarles identificadores únicos\n",
        "    for item in grafo_json[\"network\"][\"items\"]:\n",
        "        nodo_id = item[\"id\"]\n",
        "        nodos[nodo_id] = len(nodos) + 1\n",
        "        contenido_pajecket.append(f\"{nodos[nodo_id]} {item['label']}\")\n",
        "\n",
        "    # Agregar las aristas al contenido Pajek\n",
        "    contenido_pajecket.append(\"*Edges\")\n",
        "    for arista in grafo_json[\"network\"][\"links\"]:\n",
        "        source_id = arista[\"source_id\"]\n",
        "        target_id = arista[\"target_id\"]\n",
        "        source_node = nodos.get(source_id)\n",
        "        target_node = nodos.get(target_id)\n",
        "        if source_node and target_node:\n",
        "            contenido_pajecket.append(f\"{source_node} {target_node}\")\n",
        "\n",
        "    # Concatenar el contenido Pajek en una cadena\n",
        "    pajecket_string = \"\\n\".join(contenido_pajecket)\n",
        "\n",
        "    return pajecket_string\n",
        "\n",
        "\n",
        "# Nome do arquivo JSON que você deseja converter\n",
        "nome_arquivo_json = \"Maculan.json\"\n",
        "\n",
        "# Abrir e ler o conteúdo do arquivo JSON\n",
        "with open(nome_arquivo_json, \"r\") as arquivo_json:\n",
        "    grafo_json = json.load(arquivo_json)\n",
        "\n",
        "# Converter o grafo JSON para o formato Pajek\n",
        "resultado_pajek = convertir_json_a_pajecket(grafo_json)\n",
        "\n",
        "# Nome do arquivo de saída Pajek\n",
        "nome_arquivo_saida_pajek = \"grafo_Maculan.tgf\"\n",
        "\n",
        "# Salvar o resultado Pajek em um arquivo\n",
        "with open(nome_arquivo_saida_pajek, \"w\") as arquivo_saida_pajek:\n",
        "    arquivo_saida_pajek.write(resultado_pajek)\n",
        "\n",
        "print(f\"O grafo foi convertido e salvo em '{nome_arquivo_saida_pajek}' no formato Pajek.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "_cmfABS9RwZ-",
        "outputId": "f9eaea29-659a-4e5f-b64d-f39beb1ae6dc"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-7758542c9450>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Abrir e ler os arquivos dos grafos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mgraph1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabrir_arquivo_grafo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grafo_Protti1.net\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mgraph2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabrir_arquivo_grafo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grafo_Mitre1.net\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-7758542c9450>\u001b[0m in \u001b[0;36mabrir_arquivo_grafo\u001b[0;34m(nome_arquivo)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mabrir_arquivo_grafo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnome_arquivo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnome_arquivo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'grafo_Protti1.net'"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "\n",
        "def abrir_arquivo_grafo(nome_arquivo):\n",
        "    with open(nome_arquivo, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    vertices = []\n",
        "    edges = []\n",
        "    edges_started = False\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith('*Edges'):\n",
        "            edges_started = True\n",
        "            continue\n",
        "        if edges_started:\n",
        "            try:\n",
        "                source, target = map(int, line.strip().split())\n",
        "                edges.append((source, target))\n",
        "            except ValueError:\n",
        "                print(f\"Ignoring invalid edge line: {line}\")\n",
        "        else:\n",
        "            line_data = line.strip().split()\n",
        "            if len(line_data) >= 2:\n",
        "                vertex_name = ' '.join(line_data[1:])  # Ignore the vertex numbering\n",
        "                vertices.append(vertex_name)\n",
        "            else:\n",
        "                print(f\"Ignoring invalid vertex line: {line}\")\n",
        "\n",
        "    return (vertices, edges)\n",
        "\n",
        "def convert_to_nx_graph(graph_data):\n",
        "    vertices, edges = graph_data\n",
        "    G = nx.Graph()\n",
        "    for vertex in vertices:\n",
        "        G.add_node(vertex)\n",
        "    for edge in edges:\n",
        "        source, target = edge\n",
        "        G.add_edge(vertices[source - 1], vertices[target - 1])  # Subtract 1 to get 0-based indexing\n",
        "    return G\n",
        "\n",
        "def compare_graphs(graph1, graph2):\n",
        "    G1 = convert_to_nx_graph(graph1)\n",
        "    G2 = convert_to_nx_graph(graph2)\n",
        "    return nx.is_isomorphic(G1, G2)\n",
        "\n",
        "def get_common_vertices(graph1, graph2):\n",
        "    G1 = convert_to_nx_graph(graph1)\n",
        "    G2 = convert_to_nx_graph(graph2)\n",
        "    common_vertices = set(G1.nodes()).intersection(G2.nodes())\n",
        "    return common_vertices\n",
        "\n",
        "# Abrir e ler os arquivos dos grafos\n",
        "graph1 = abrir_arquivo_grafo(\"grafo_Protti1.net\")\n",
        "graph2 = abrir_arquivo_grafo(\"grafo_Mitre1.net\")\n",
        "print(graph1)\n",
        "# Comparar os dois grafos\n",
        "result = compare_graphs(graph1, graph2)\n",
        "\n",
        "# Imprimir o resultado da comparação\n",
        "if result:\n",
        "    print(\"Os grafos são iguais.\")\n",
        "else:\n",
        "    print(\"Os grafos são diferentes.\")\n",
        "\n",
        "# Obter e imprimir os vértices comuns\n",
        "common_vertices = get_common_vertices(graph1, graph2)\n",
        "print(\"Vértices comuns nos dois grafos:\")\n",
        "for vertex in common_vertices:\n",
        "    print(vertex)\n",
        "\n",
        "def merge_graphs(graph1, graph2):\n",
        "    # Create a new graph to store the merged graph\n",
        "    merged_graph = {\n",
        "        'vertices': {},\n",
        "        'edges': []\n",
        "    }\n",
        "\n",
        "    # Add vertices from the first graph to the merged graph\n",
        "    for vertex in graph1[0]:\n",
        "        merged_graph['vertices'][vertex] = len(merged_graph['vertices']) + 1\n",
        "\n",
        "    # Add vertices from the second graph that are not common to the merged graph\n",
        "    for vertex in graph2[0]:\n",
        "        if vertex not in merged_graph['vertices']:\n",
        "            merged_graph['vertices'][vertex] = len(merged_graph['vertices']) + 1\n",
        "\n",
        "    # Identify common vertices and choose a unique name for them\n",
        "    common_vertices = set(graph1[0]).intersection(graph2[0])\n",
        "    common_vertex_mapping = {}\n",
        "    for vertex in common_vertices:\n",
        "        common_vertex_mapping[vertex] = len(merged_graph['vertices']) + 1\n",
        "        merged_graph['vertices'][vertex] = len(merged_graph['vertices']) + 1\n",
        "\n",
        "    # Add edges from the first graph to the merged graph, replacing common vertices\n",
        "    for edge in graph1[1]:\n",
        "        source, target = edge\n",
        "        if source in common_vertex_mapping:\n",
        "            source = common_vertex_mapping[source]\n",
        "        if target in common_vertex_mapping:\n",
        "            target = common_vertex_mapping[target]\n",
        "        merged_graph['edges'].append((source, target))\n",
        "\n",
        "    # Add edges from the second graph to the merged graph, replacing common vertices\n",
        "    for edge in graph2[1]:\n",
        "        source, target = edge\n",
        "        if source in common_vertex_mapping:\n",
        "            source = common_vertex_mapping[source]\n",
        "        if target in common_vertex_mapping:\n",
        "            target = common_vertex_mapping[target]\n",
        "        merged_graph['edges'].append((source, target))\n",
        "\n",
        "    return merged_graph\n",
        "\n",
        "merged_graph = merge_graphs(graph1, graph2)\n",
        "print(merged_graph)\n",
        "\n",
        "nome_arquivo_saida_pajek = \"grafo_mesclado2.net\"\n",
        "with open(nome_arquivo_saida_pajek, \"w\") as arquivo_saida_pajek:\n",
        "    arquivo_saida_pajek.write(\"*Vertices\\n\")\n",
        "    for i, vertex in enumerate(merged_graph['vertices'], 1):\n",
        "        arquivo_saida_pajek.write(f\"{i} {vertex}\\n\")\n",
        "\n",
        "    arquivo_saida_pajek.write(\"*Edges\\n\")\n",
        "    for edge in merged_graph['edges']:\n",
        "        source, target = edge\n",
        "        arquivo_saida_pajek.write(f\"{source} {target}\\n\")\n",
        "\n",
        "print(f\"O grafo foi convertido e salvo em '{nome_arquivo_saida_pajek}' no formato Pajek.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CPgxFx8pBKo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDtfF39lx4mFQZndIHlksV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}